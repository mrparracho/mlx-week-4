{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LOAD CLIP's Visual Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import CLIPVisionModel, CLIPImageProcessor\n",
    "from PIL import Image\n",
    "\n",
    "model_name = 'openai/clip-vit-base-patch32'\n",
    "\n",
    "# Image Processor (for resizing, normalization)\n",
    "processor = CLIPImageProcessor.from_pretrained(model_name)\n",
    "\n",
    "# Visual Encoder (Patch-level outputs)\n",
    "clip_vision_encoder = CLIPVisionModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract Image Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 50, 768])\n",
      "tensor([[[-0.6340,  0.3656, -0.2445,  ..., -0.0313,  0.3453,  0.0162],\n",
      "         [-0.1868,  0.2712, -0.6086,  ...,  0.0164,  0.0588,  0.1814],\n",
      "         [-0.0984,  0.2395, -0.6235,  ..., -0.0471,  0.3991,  0.2777],\n",
      "         ...,\n",
      "         [-0.3309, -0.3027, -0.2398,  ...,  0.4375,  0.6725, -0.0038],\n",
      "         [-0.2847,  0.0317, -0.2260,  ...,  0.2764,  0.4962, -0.0597],\n",
      "         [ 0.4559,  0.5215, -1.1170,  ...,  0.2762,  0.4585, -0.2910]]])\n"
     ]
    }
   ],
   "source": [
    "image = Image.open('my_image.jpg')\n",
    "inputs = processor(images=image, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = clip_vision_encoder(**inputs)\n",
    "\n",
    "# Extract patch-level embeddings (not pooled!)\n",
    "image_embeds = outputs.last_hidden_state  # shape: [1, num_patches+1, hidden_dim]\n",
    "print(image_embeds.shape)\n",
    "print(image_embeds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Transformer Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39b0b7c7978848d9b6d66012a99c64d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "714ec6f371114135bd6553663bc3b3d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "031e91459c034fc6a5d4a0814ac5eda3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "598cbca4df8f471c88ba73eb69a981b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5ad6f182c2c45babb08235150a62e17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2LMHeadModel(\n",
      "  (transformer): GPT2Model(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(1024, 768)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D(nf=2304, nx=768)\n",
      "          (c_proj): Conv1D(nf=768, nx=768)\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (crossattention): GPT2Attention(\n",
      "          (c_attn): Conv1D(nf=1536, nx=768)\n",
      "          (q_attn): Conv1D(nf=768, nx=768)\n",
      "          (c_proj): Conv1D(nf=768, nx=768)\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_cross_attn): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D(nf=3072, nx=768)\n",
      "          (c_proj): Conv1D(nf=768, nx=3072)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, GPT2Config\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Modify GPT-2 to accept Cross-Attention\n",
    "config = GPT2Config.from_pretrained('gpt2')\n",
    "config.add_cross_attention = True\n",
    "\n",
    "decoder = GPT2LMHeadModel(config)\n",
    "\n",
    "print(decoder)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`generation_config` default values have been modified to match model-specific defaults: {'bos_token_id': 50256, 'eos_token_id': 50256}. If this is not desired, please set these values explicitly.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated caption: A photo of Hawai Hawai reperc reperc reperc reperc protecting protecting protecting reperc repercituituitu accommodate accommodate accommodate Sega Sega Sega\n"
     ]
    }
   ],
   "source": [
    "from transformers import GenerationConfig\n",
    "\n",
    "\n",
    "generation_config = GenerationConfig(\n",
    "    max_length=20,\n",
    "    num_beams=5,\n",
    "    early_stopping=True,\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "input_ids = tokenizer(\"A photo of\", return_tensors=\"pt\").input_ids\n",
    "\n",
    "# Generate caption conditioned on image embeddings\n",
    "outputs = decoder(input_ids=input_ids, encoder_hidden_states=image_embeds)\n",
    "\n",
    "# Get logits to predict next token\n",
    "# logits = outputs.logits\n",
    "# predicted_next_token = torch.argmax(logits[:, -1, :], dim=-1)\n",
    "# print(predicted_next_token.)\n",
    "\n",
    "with torch.no_grad():\n",
    "    generated_ids = decoder.generate(\n",
    "        input_ids=input_ids,\n",
    "        encoder_hidden_states=image_embeds,\n",
    "        generation_config=generation_config\n",
    "        \n",
    "    )\n",
    "\n",
    "caption = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "print(\"Generated caption:\", caption)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(decoder.parameters(), lr=1e-4)\n",
    "\n",
    "\n",
    "\n",
    "for images, captions in dataloader:\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    inputs = processor(images=images, return_tensors=\"pt\")\n",
    "    image_embeds = clip_vision_encoder(**inputs).last_hidden_state\n",
    "\n",
    "    caption_inputs = tokenizer(captions, padding=True, return_tensors=\"pt\")\n",
    "\n",
    "    outputs = decoder(\n",
    "        input_ids=caption_inputs.input_ids,\n",
    "        attention_mask=caption_inputs.attention_mask,\n",
    "        encoder_hidden_states=image_embeds,\n",
    "        labels=caption_inputs.input_ids\n",
    "    )\n",
    "\n",
    "    loss = outputs.loss\n",
    "    loss.backward()\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Caption Generation (Inference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GenerationConfig\n",
    "\n",
    "generation_config = GenerationConfig(\n",
    "    max_length=20,\n",
    "    num_beams=5,\n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "generated_ids = decoder.generate(\n",
    "    input_ids=tokenizer(\"A photo of\", return_tensors=\"pt\").input_ids,\n",
    "    encoder_hidden_states=image_embeds,\n",
    "    generation_config=generation_config\n",
    ")\n",
    "\n",
    "caption = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "print(caption)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated caption: A photo of restoration restoration restoration restoration deposit deposit deposit KNOW KNOW KNOW KNOW KNOW KNOW KNOW Counsel Counsel Counsel Counsel Counsel Counsel\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import CLIPVisionModel, CLIPImageProcessor, GPT2Tokenizer, GPT2LMHeadModel, GPT2Config, GenerationConfig\n",
    "from PIL import Image\n",
    "\n",
    "# 1. Load models and processor\n",
    "model_name = 'openai/clip-vit-base-patch32'\n",
    "processor = CLIPImageProcessor.from_pretrained(model_name)\n",
    "clip_vision_encoder = CLIPVisionModel.from_pretrained(model_name)\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "config = GPT2Config.from_pretrained('gpt2')\n",
    "config.add_cross_attention = True\n",
    "decoder = GPT2LMHeadModel(config)\n",
    "\n",
    "# 2. Load and preprocess the image\n",
    "image = Image.open(\"my_image.jpg\")\n",
    "inputs = processor(images=image, return_tensors=\"pt\")\n",
    "\n",
    "# 3. Get image embeddings\n",
    "with torch.no_grad():\n",
    "    outputs = clip_vision_encoder(**inputs)\n",
    "image_embeds = outputs.last_hidden_state  # shape: [1, num_patches+1, hidden_dim]\n",
    "\n",
    "# 4. Generate caption\n",
    "generation_config = GenerationConfig(\n",
    "    max_length=20,\n",
    "    num_beams=5,\n",
    "    early_stopping=True,\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "prompt = \"A photo of\"\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "\n",
    "with torch.no_grad():\n",
    "    generated_ids = decoder.generate(\n",
    "        input_ids=input_ids,\n",
    "        encoder_hidden_states=image_embeds,\n",
    "        generation_config=generation_config\n",
    "    )\n",
    "\n",
    "caption = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "print(\"Generated caption:\", caption)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38e564aad0954012b9aedcd8e756415c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0797ddb5f1564135a63dca7933e732ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/982M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cfd6ec2a2f14a82a38d6c37be825e8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/982M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3d7431a54b94316be1f606a1ef537b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/228 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee9ddd33eefa49f288cb5d8049d14a20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/241 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e16bbe7c3b74f77a781bf07671edc36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42a6aa2345e4462784469eb39f064f6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba5513968e284fcba75fe48b7232a561",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36184fc22b424aa781e67d5bf29d0c3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/120 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n",
      "You may ignore this warning if your `pad_token_id` (50256) is identical to the `bos_token_id` (50256), `eos_token_id` (50256), or the `sep_token_id` (None), and your input is not padded.\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "Make sure that a `_reorder_cache` function is correctly implemented in transformers.models.gpt2.modeling_gpt2 to enable beam search for <class 'transformers.models.gpt2.modeling_gpt2.GPT2LMHeadModel'>",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNotImplementedError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      9\u001b[39m image = Image.open(\u001b[33m\"\u001b[39m\u001b[33mmy_image.jpg\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     10\u001b[39m pixel_values = feature_extractor(images=image, return_tensors=\u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m).pixel_values\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m output_ids = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_beams\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m caption = tokenizer.batch_decode(output_ids, skip_special_tokens=\u001b[38;5;28;01mTrue\u001b[39;00m)[\u001b[32m0\u001b[39m]\n\u001b[32m     14\u001b[39m \u001b[38;5;28mprint\u001b[39m(caption)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git/mlx-week-4/.venv/lib/python3.11/site-packages/torch/utils/_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git/mlx-week-4/.venv/lib/python3.11/site-packages/transformers/generation/utils.py:2642\u001b[39m, in \u001b[36mGenerationMixin.generate\u001b[39m\u001b[34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[39m\n\u001b[32m   2635\u001b[39m     input_ids, model_kwargs = \u001b[38;5;28mself\u001b[39m._expand_inputs_for_generation(\n\u001b[32m   2636\u001b[39m         input_ids=input_ids,\n\u001b[32m   2637\u001b[39m         expand_size=generation_config.num_beams,\n\u001b[32m   2638\u001b[39m         is_encoder_decoder=\u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder,\n\u001b[32m   2639\u001b[39m         **model_kwargs,\n\u001b[32m   2640\u001b[39m     )\n\u001b[32m   2641\u001b[39m     \u001b[38;5;66;03m# 12. run beam sample\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2642\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_beam_search\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2643\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2644\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2645\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2646\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2647\u001b[39m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m=\u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2648\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2649\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2651\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m generation_mode == GenerationMode.GROUP_BEAM_SEARCH:\n\u001b[32m   2652\u001b[39m     logger.warning_once(\n\u001b[32m   2653\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mGroup Beam Search is scheduled to be moved to a `custom_generate` repository in v4.55.0. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2654\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mTo prevent loss of backward compatibility, add `trust_remote_code=True` to your `generate` call.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2655\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git/mlx-week-4/.venv/lib/python3.11/site-packages/transformers/generation/utils.py:4183\u001b[39m, in \u001b[36mGenerationMixin._beam_search\u001b[39m\u001b[34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, **model_kwargs)\u001b[39m\n\u001b[32m   4178\u001b[39m \u001b[38;5;66;03m# g. Prepare remaining data for the next iteration, including computing the stopping condition for\u001b[39;00m\n\u001b[32m   4179\u001b[39m \u001b[38;5;66;03m# beam search as a whole (as opposed to individual beams, i.e. `stopping_criteria`)\u001b[39;00m\n\u001b[32m   4180\u001b[39m \n\u001b[32m   4181\u001b[39m \u001b[38;5;66;03m# pluck the cache from the beam indices that will be used in the next iteration\u001b[39;00m\n\u001b[32m   4182\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m model_kwargs.get(\u001b[33m\"\u001b[39m\u001b[33mpast_key_values\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m4183\u001b[39m     model_kwargs[\u001b[33m\"\u001b[39m\u001b[33mpast_key_values\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_temporary_reorder_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4184\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpast_key_values\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4185\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbeam_idx\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_flatten_beam_dim\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrunning_beam_indices\u001b[49m\u001b[43m[\u001b[49m\u001b[43m.\u001b[49m\u001b[43m.\u001b[49m\u001b[43m.\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcur_len\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder_prompt_len\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4186\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4188\u001b[39m cur_len = cur_len + \u001b[32m1\u001b[39m\n\u001b[32m   4189\u001b[39m this_peer_finished = \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._beam_search_has_unfinished_sequences(\n\u001b[32m   4190\u001b[39m     running_beam_scores,\n\u001b[32m   4191\u001b[39m     beam_scores,\n\u001b[32m   (...)\u001b[39m\u001b[32m   4198\u001b[39m     length_penalty,\n\u001b[32m   4199\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git/mlx-week-4/.venv/lib/python3.11/site-packages/transformers/generation/utils.py:3709\u001b[39m, in \u001b[36mGenerationMixin._temporary_reorder_cache\u001b[39m\u001b[34m(self, past_key_values, beam_idx)\u001b[39m\n\u001b[32m   3707\u001b[39m \u001b[38;5;66;03m# Exception 1: code path for models using the legacy cache format\u001b[39;00m\n\u001b[32m   3708\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(past_key_values, (\u001b[38;5;28mtuple\u001b[39m, \u001b[38;5;28mlist\u001b[39m)):\n\u001b[32m-> \u001b[39m\u001b[32m3709\u001b[39m     past_key_values = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_reorder_cache\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeam_idx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3710\u001b[39m \u001b[38;5;66;03m# Exception 2: models with different cache formats. These are limited to `DynamicCache` until their\u001b[39;00m\n\u001b[32m   3711\u001b[39m \u001b[38;5;66;03m# cache format is standardized, to avoid adding complexity to the codebase.\u001b[39;00m\n\u001b[32m   3712\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mgptbigcode\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m model_class:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git/mlx-week-4/.venv/lib/python3.11/site-packages/transformers/models/vision_encoder_decoder/modeling_vision_encoder_decoder.py:602\u001b[39m, in \u001b[36mVisionEncoderDecoderModel._reorder_cache\u001b[39m\u001b[34m(self, past_key_values, beam_idx)\u001b[39m\n\u001b[32m    600\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_reorder_cache\u001b[39m(\u001b[38;5;28mself\u001b[39m, past_key_values, beam_idx):\n\u001b[32m    601\u001b[39m     \u001b[38;5;66;03m# apply decoder cache reordering here\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m602\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdecoder\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_reorder_cache\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeam_idx\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git/mlx-week-4/.venv/lib/python3.11/site-packages/transformers/generation/utils.py:1013\u001b[39m, in \u001b[36mGenerationMixin._reorder_cache\u001b[39m\u001b[34m(self, past_key_values, beam_idx)\u001b[39m\n\u001b[32m   1012\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_reorder_cache\u001b[39m(\u001b[38;5;28mself\u001b[39m, past_key_values, beam_idx):\n\u001b[32m-> \u001b[39m\u001b[32m1013\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[32m   1014\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMake sure that a `_reorder_cache` function is correctly implemented in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__module__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m to\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1015\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m enable beam search for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   1016\u001b[39m     )\n",
      "\u001b[31mNotImplementedError\u001b[39m: Make sure that a `_reorder_cache` function is correctly implemented in transformers.models.gpt2.modeling_gpt2 to enable beam search for <class 'transformers.models.gpt2.modeling_gpt2.GPT2LMHeadModel'>"
     ]
    }
   ],
   "source": [
    "from transformers import VisionEncoderDecoderModel, ViTImageProcessor, AutoTokenizer\n",
    "\n",
    "# Load pre-trained image captioning model\n",
    "model = VisionEncoderDecoderModel.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
    "feature_extractor = ViTImageProcessor.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
    "\n",
    "# Generate caption\n",
    "image = Image.open(\"my_image.jpg\")\n",
    "pixel_values = feature_extractor(images=image, return_tensors=\"pt\").pixel_values\n",
    "\n",
    "output_ids = model.generate(pixel_values, max_length=50, num_beams=4)\n",
    "caption = tokenizer.batch_decode(output_ids, skip_special_tokens=True)[0]\n",
    "print(caption)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
